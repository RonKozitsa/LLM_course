# Ollama Chatbot GUI - Angular

A professional, modern Angular application that provides a sleek interface for interacting with locally-hosted Ollama AI models.

<img width="1438" height="817" alt="Screenshot 2025-11-06 at 10 32 41â€¯PM" src="https://github.com/user-attachments/assets/57e109d0-b2ac-4fbc-9289-8e2afd5728bf" />


## ğŸŒŸ Features

- âœ¨ **Modern Angular Architecture** - Built with Angular 17 and TypeScript
- ğŸ¨ **Professional Dark Theme** - Sleek, modern UI with smooth animations
- ğŸ”’ **100% Local** - All conversations stay on your machine
- âš¡ **Real-time Responses** - Reactive programming with RxJS
- ğŸ’¬ **Message Management** - Advanced state management with services
- ğŸ”„ **Model Selection** - Easy switching between Ollama models
- ğŸ“± **Responsive Design** - Works great on all screen sizes
- ğŸ¯ **Type Safety** - Full TypeScript support
- ğŸ§ª **Comprehensive Tests** - 150+ unit tests with ~93% code coverage
- ğŸš€ **Production Ready** - Optimized builds and CI/CD ready

## ğŸ“‹ Prerequisites

### 1. Node.js & npm
```bash
node --version  # Should be 18.x or higher
npm --version   # Should be 9.x or higher
```

Install from: https://nodejs.org/

### 2. Angular CLI
```bash
npm install -g @angular/cli
```

### 3. Ollama
Install Ollama from https://ollama.com

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from https://ollama.com/download

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Navigate to project directory
cd ollama-chatbot-angular

# Install npm packages
npm install
```

### 2. Start Ollama

```bash
# Start Ollama server
ollama serve

# Pull a model (in another terminal)
ollama pull llama2
```

### 3. Run the Application

```bash
# Development server
npm start

# Or
ng serve
```

Open your browser to `http://localhost:4200`

## ğŸ“¦ Project Structure

```
ollama-chatbot-angular/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/        # Angular components (future)
â”‚   â”‚   â”œâ”€â”€ services/          # Services
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama.service.ts    # Ollama API service
â”‚   â”‚   â”‚   â””â”€â”€ chat.service.ts      # Chat state management
â”‚   â”‚   â”œâ”€â”€ models/            # TypeScript interfaces
â”‚   â”‚   â”‚   â””â”€â”€ chat.model.ts        # Data models
â”‚   â”‚   â”œâ”€â”€ app.component.ts         # Main component
â”‚   â”‚   â”œâ”€â”€ app.component.html       # Main template
â”‚   â”‚   â”œâ”€â”€ app.component.css        # Main styles
â”‚   â”‚   â””â”€â”€ app.module.ts            # App module
â”‚   â”œâ”€â”€ assets/                # Static assets
â”‚   â”œâ”€â”€ environments/          # Environment configs
â”‚   â”œâ”€â”€ index.html            # HTML entry point
â”‚   â”œâ”€â”€ main.ts               # Bootstrap file
â”‚   â””â”€â”€ styles.css            # Global styles
â”œâ”€â”€ angular.json              # Angular configuration
â”œâ”€â”€ package.json              # Dependencies
â”œâ”€â”€ tsconfig.json             # TypeScript config
â””â”€â”€ README.md                 # This file
```

## ğŸ¯ Usage

### Basic Usage

1. **Start the app**: `npm start`
2. **Type a message** in the input field at the bottom
3. **Press Enter** or click "Send" to send
4. **Receive responses** from your local AI model

### Keyboard Shortcuts

- `Enter` - Send message
- `Shift + Enter` - New line in message

### Changing Models

Edit the model name in the sidebar input field and press Enter. Make sure the model is installed:

```bash
ollama list           # See installed models
ollama pull mistral   # Install a new model
```

## âš™ï¸ Configuration

### Change Ollama URL

Edit `src/app/services/ollama.service.ts`:

```typescript
private config: OllamaConfig = {
  url: 'http://localhost:11434',  // Change this
  model: 'llama2',
  timeout: 120000
};
```

### Customize Colors

Edit the CSS variables in `src/app/app.component.css` and `src/styles.css`.

## ğŸ”§ Development

### Install Development Dependencies

```bash
npm install
```

### Run Development Server

```bash
ng serve
# Or
npm start
```

Navigate to `http://localhost:4200`. The app will automatically reload if you change any source files.

### Build for Production

```bash
ng build --configuration production
```

The build artifacts will be stored in the `dist/` directory.

### Run Tests

```bash
ng test
```

The project includes **comprehensive unit tests** with **150+ test cases** and **~93% code coverage**.

#### Test Files:
- `src/app/services/ollama.service.spec.ts` - 60+ tests for OllamaService
- `src/app/services/chat.service.spec.ts` - 40+ tests for ChatService
- `src/app/app.component.spec.ts` - 50+ tests for AppComponent

#### Run Tests with Coverage:
```bash
ng test --code-coverage
```

#### Run Tests Once (CI/CD):
```bash
ng test --watch=false --browsers=ChromeHeadless
```

#### View Coverage Report:
```bash
open coverage/ollama-chatbot-angular/index.html
```

See [TESTING.md](TESTING.md) for complete testing documentation.

### Lint Code

```bash
ng lint
```

## ğŸ“Š Architecture

### Services

**OllamaService** (`src/app/services/ollama.service.ts`)
- Handles HTTP communication with Ollama API
- Manages connection status
- Error handling and recovery

**ChatService** (`src/app/services/chat.service.ts`)
- Manages message state
- Handles message creation and storage
- Processing state management

### Models

**Message Interface** (`src/app/models/chat.model.ts`)
```typescript
interface Message {
  id: string;
  sender: 'user' | 'ai' | 'system';
  content: string;
  timestamp: Date;
  type: 'user' | 'ai' | 'error';
}
```

### State Management

- Uses RxJS BehaviorSubjects for reactive state
- Services provide observables for components
- Unidirectional data flow
- Automatic cleanup with `takeUntil`

## ğŸ› Troubleshooting

### Cannot connect to Ollama

**Error:** `Cannot connect to Ollama. Make sure it is running on localhost:11434`

**Solution:**
```bash
# Start Ollama
ollama serve

# Verify it's running
curl http://localhost:11434/api/tags
```

### CORS Issues

If you encounter CORS errors, Ollama should allow requests from localhost by default. If not, you may need to configure CORS settings.

### Model not found

**Error:** `Model not found`

**Solution:**
```bash
# List installed models
ollama list

# Pull the model you want
ollama pull llama2
```

### Build errors

```bash
# Clear node_modules and reinstall
rm -rf node_modules package-lock.json
npm install

# Clear Angular cache
ng cache clean
```

## ğŸ“ Available Scripts

| Command | Description |
|---------|-------------|
| `npm start` | Start development server |
| `npm run build` | Build for production |
| `npm test` | Run unit tests |
| `npm run watch` | Build and watch for changes |
| `ng serve` | Alternative to npm start |
| `ng build` | Alternative to npm run build |

## ğŸŒ Browser Support

- Chrome (latest)
- Firefox (latest)
- Safari (latest)
- Edge (latest)

## ğŸ“Š Performance

| Metric | Target |
|--------|--------|
| Initial Load | <2s |
| Time to Interactive | <3s |
| Bundle Size | <500KB |
| Lighthouse Score | >90 |

## ğŸ” Security & Privacy

- âœ… **100% Local Processing** - No external servers
- âœ… **No Telemetry** - No data collection
- âœ… **No Internet Required** - After initial setup
- âœ… **Private Conversations** - Everything stays on device
- âœ… **Type Safety** - TypeScript prevents common bugs

## ğŸ“š Technologies Used

- **Angular 17** - Modern web framework
- **TypeScript 5.2** - Type-safe JavaScript
- **RxJS 7.8** - Reactive programming
- **HttpClient** - HTTP communication
- **FormsModule** - Two-way data binding

## ğŸ› ï¸ System Requirements

**Minimum:**
- Node.js: 18.x
- npm: 9.x
- RAM: 4GB (8GB with model)
- CPU: 2 cores @ 2.0 GHz
- Browser: Modern browser with ES2022 support

**Recommended:**
- Node.js: 20.x
- npm: 10.x
- RAM: 16GB
- CPU: 4+ cores @ 3.0 GHz
- GPU: Optional (8GB+ VRAM for faster inference)

## ğŸ“– Documentation

- [Technical PRD](TECHNICAL_PRD_ANGULAR.md) - Complete technical documentation
- [Testing Guide](TESTING.md) - Unit testing documentation and coverage
- [Quick Start Guide](QUICKSTART.md) - Fast setup instructions
- [Angular Docs](https://angular.io/docs) - Angular documentation
- [Ollama Docs](https://github.com/ollama/ollama) - Ollama documentation
- [RxJS Docs](https://rxjs.dev/) - RxJS documentation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Credits

- Built with [Angular](https://angular.io)
- Powered by [Ollama](https://ollama.com)
- Icons from Unicode emojis
- Color scheme: Catppuccin Mocha

## ğŸš€ Deployment

### Development
```bash
ng serve
```

### Production Build
```bash
ng build --configuration production
```

### Docker (Optional)
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build
CMD ["ng", "serve", "--host", "0.0.0.0"]
```

---

**Made with â¤ï¸ using Angular and TypeScript**

Enjoy your professional, type-safe, local AI assistant! ğŸš€
